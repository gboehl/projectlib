{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HANK estimation including household parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook replicates the estimation of the medium-scale HANK model from [Ensemble MCMC Sampling for DSGE Models](https://gregorboehl.com/live/dime_mcmc_boehl.pdf). This is the estimation with the smaller grid, but the estimation of the model with the larger grid is exactly the same. Please refer to the original paper for details.\n",
    "\n",
    "Let's start with a few imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import pathos\n",
    "import emcee\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import emcwrap as ew\n",
    "import grgrlib.hanktools as htools\n",
    "\n",
    "# make everything reproducible\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is sourced out to an external file. Go get it. Its in the ``ressources`` folder if you want to get a closer look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = htools.load_model('ressources/hank2_small.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let us prepare the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data and set the correct time indices\n",
    "d0 = pd.read_csv('ressources/BS_data.csv', sep=';', index_col='date', parse_dates=True).dropna()\n",
    "d0.index = pd.date_range('1973Q1', periods=len(d0.index), freq='Q')\n",
    "\n",
    "# select desired time series\n",
    "series = ['GDP', 'Infl', 'FFR', 'Cons_JPT', 'Lab', 'Inv_JPT', 'Wage']\n",
    "data = d0[series]['1983Q1':'2008Q4'].to_numpy()\n",
    "\n",
    "# set measurement errors as a fraction of the standard deviation of the time series\n",
    "me_sig = np.std(data, axis=0)*1e-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need the model's initial steady state as a reference point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial SS took 5.074892044067383 seconds.\n"
     ]
    }
   ],
   "source": [
    "st = time.time()\n",
    "hank_ss, ss, unknowns_ss, targets_ss, hank, unknowns, targets, exogenous = model.dag()\n",
    "print(f\"Initial SS took {time.time() - st} seconds.\")\n",
    "\n",
    "# put this in a dictionary for later\n",
    "jac_info = {'unknowns': unknowns, 'targets': targets, 'exogenous': exogenous, 'T': 300, 'ss': ss}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All necessary information for the estimation is expressed in a yaml file. Lets extract everything we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   adding Z_AR_COEF...\n",
      "   adding rstar_AR_COEF...\n",
      "   adding G_AR_COEF...\n",
      "   adding markup_w_AR_COEF...\n",
      "   adding markup_AR_COEF...\n",
      "   adding rinv_shock_AR_COEF...\n",
      "   adding beta_AR_COEF...\n",
      "   adding Z_SIG_COEF...\n",
      "   adding rstar_SIG_COEF...\n",
      "   adding G_SIG_COEF...\n",
      "   adding markup_w_SIG_COEF...\n",
      "   adding markup_SIG_COEF...\n",
      "   adding rinv_shock_SIG_COEF...\n",
      "   adding beta_SIG_COEF...\n",
      "Adding parameters to the prior distribution...\n",
      "   - sig_c as normal with mean 1.5 and std/df 0.375\n",
      "   - sig_l as normal with mean 2.0 and std/df 0.75\n",
      "   - chi0 as gamma with mean 0.2 and std/df 0.15\n",
      "   - tau as beta with mean 0.2 and std/df 0.1\n",
      "   - sigma_z as normal with mean 1.0 and std/df 0.4\n",
      "   - phiss as gamma with mean 4.0 and std/df 2.0\n",
      "   - zeta_p as beta with mean 0.5 and std/df 0.1\n",
      "   - zeta_w as beta with mean 0.5 and std/df 0.1\n",
      "   - iota_p as beta with mean 0.5 and std/df 0.15\n",
      "   - iota_w as beta with mean 0.5 and std/df 0.15\n",
      "   - phi_pi as gamma with mean 1.5 and std/df 0.25\n",
      "   - phi_y as gamma with mean 0.125 and std/df 0.05\n",
      "   - rho as beta with mean 0.75 and std/df 0.1\n",
      "   - ybar as normal with mean 0.4 and std/df 0.1\n",
      "   - nbar as normal with mean 0.0 and std/df 2.0\n",
      "   - pistar as gamma with mean 0.625 and std/df 0.1\n",
      "   - rstar as gamma with mean 1.25 and std/df 0.1\n",
      "   - Z_AR_COEF as beta with mean 0.5 and std/df 0.2\n",
      "   - rstar_AR_COEF as beta with mean 0.5 and std/df 0.2\n",
      "   - G_AR_COEF as beta with mean 0.5 and std/df 0.2\n",
      "   - markup_w_AR_COEF as beta with mean 0.5 and std/df 0.2\n",
      "   - markup_AR_COEF as beta with mean 0.5 and std/df 0.2\n",
      "   - rinv_shock_AR_COEF as beta with mean 0.5 and std/df 0.2\n",
      "   - beta_AR_COEF as beta with mean 0.5 and std/df 0.2\n",
      "   - Z_SIG_COEF as inv_gamma with mean 0.1 and std/df 0.25\n",
      "   - rstar_SIG_COEF as inv_gamma with mean 0.1 and std/df 0.25\n",
      "   - G_SIG_COEF as inv_gamma with mean 0.1 and std/df 0.25\n",
      "   - markup_w_SIG_COEF as inv_gamma with mean 0.1 and std/df 0.25\n",
      "   - markup_SIG_COEF as inv_gamma with mean 0.1 and std/df 0.25\n",
      "   - rinv_shock_SIG_COEF as inv_gamma with mean 0.1 and std/df 0.25\n",
      "   - beta_SIG_COEF as inv_gamma with mean 0.1 and std/df 0.25\n"
     ]
    }
   ],
   "source": [
    "# load yaml\n",
    "est_info = ew.parse_yaml('ressources/hank2.yaml')\n",
    "\n",
    "# get priors from the yaml\n",
    "prior = est_info['estimation']['prior']\n",
    "shocks = est_info['declarations']['shocks']\n",
    "observables = est_info['declarations']['observables']\n",
    "\n",
    "# compile priors\n",
    "frozen_prior, prior_func, bptrans, _, _, prior = htools.get_prior(prior, shocks, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step is to define all relevant functions for the estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_func(x, ss, data):\n",
    "    \"\"\"Remove intercept from series\n",
    "    \"\"\"\n",
    "\n",
    "    data_adj = np.empty_like(data)\n",
    "    data_adj[:, 0] = data[:, 0] - ss['ybar']  # y\n",
    "    data_adj[:, 1] = data[:, 1] - ss['pistar']  # pi\n",
    "    data_adj[:, 2] = data[:, 2] - ss['rstar']  # i\n",
    "    data_adj[:, 3] = data[:, 3] - ss['ybar']  # c\n",
    "    data_adj[:, 4] = data[:, 4] - ss['n_obs']  # n\n",
    "    data_adj[:, 5] = data[:, 5] - ss['ybar']  # I\n",
    "    data_adj[:, 6] = data[:, 6] - ss['ybar']  # w\n",
    "\n",
    "    return data_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ss_func(ss, x):\n",
    "    \"\"\"Calculate steady state\n",
    "    \"\"\"\n",
    "\n",
    "    ss['pi'] = ss['pistar']/100\n",
    "    ss['i'] = ss['rstar']/100\n",
    "    ss['r'] = (1 + ss['i']) / (1 + ss['pi']) - 1\n",
    "\n",
    "    # the actual function to calculate the steady state\n",
    "    ss = hank_ss.solve_steady_state(ss, unknowns_ss, targets_ss, solver=\"hybr\")\n",
    "    ss = hank.steady_state(ss)\n",
    "\n",
    "    return ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_ll(x):\n",
    "    \"\"\"A single posterior density evaluation\n",
    "    \"\"\" \n",
    "    \n",
    "    x = bptrans(x)\n",
    "    x = np.array(x)\n",
    "\n",
    "    # check prior first, and exit already if infinity\n",
    "    lprior = prior_func(x)\n",
    "    if np.isinf(lprior):\n",
    "        return -np.inf\n",
    "\n",
    "    # calculate likelihood\n",
    "    llike, ss_local = htools.get_ll(x, hank, data, data_func, me_sig, jac_info, list(prior), observables, shocks, ss_func=lambda ss, x: ss_func(ss, x), debug=False)\n",
    "\n",
    "    # exit if unsuccessful\n",
    "    if ss_local is None:\n",
    "        return -np.inf\n",
    "    return llike + lprior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, set some parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of chains. This machine has 48 cores.\n",
    "nchain = 48*4\n",
    "# number of iterations\n",
    "nsteps = 2000\n",
    "\n",
    "# initialize DIME MCMC\n",
    "move = ew.DIMEMove(aimh_prob=0.1)\n",
    "# decide where to store the results\n",
    "backend_name = 'hank_small0.h5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last thing we need is a sample from the prior to initialize our chains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/192 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# disable warnings. Bad likelihood draws are dealt by the log_ll func\n",
    "warnings.filterwarnings('ignore')\n",
    "# initialize a parallel pool. I like pathos\n",
    "pool = pathos.pools.ProcessPool()\n",
    "\n",
    "# get a prior sample (in parameter space)\n",
    "p0 = ew.get_prior_sample(frozen_prior, nchain, mapper=pool.uimap, check_func=lambda x: log_ll(bptrans(x, False)), debug=False)\n",
    "# transform into proposal space\n",
    "p0pspace = bptrans(p0, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it. We can finally start sampling..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the storage\n",
    "backend = emcee.backends.HDFBackend(backend_name)\n",
    "\n",
    "# sample the sampler\n",
    "sampler = ew.run_mcmc(log_ll, p0=p0pspace, nsteps=nsteps, moves=move, tune=500, priors=prior, prior_transform=bptrans, backend=backend, update_freq=100, pool=pool, maintenance_interval=10)\n",
    "\n",
    "# also save some additional infos\n",
    "ew.save_to_backend(sampler, {'tune': 500, 'priors': list(prior)})"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
